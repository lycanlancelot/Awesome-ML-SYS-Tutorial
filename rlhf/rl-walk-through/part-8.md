# 模仿学习

虽然强化学习不需要有监督学习中的标签数据，但它十分依赖奖励函数的设置。有时在奖励函数上做一些微小的改动，训练出来的策略就会有天差地别。在很多现实场景中，奖励函数并不是一目了然的，奖励函数的设计需要大量的试错和调试过程。幸运的是，我们可以利用人类专家的策略来引导智能体的策略训练，从而加速训练过程。基于这种思想，我们可以从人类专家的轨迹中提取出策略的知识，供智能体学习。

我们可以将这一过程看作是模仿学习 (imitation learning) 研究的模型这一类问题，在模仿学习的框架下，专家能够提供一系列状态动作对 $\{(s_t, a_t)\}$，表示专家在 $s_t$ 状态下采取的动作 $a_t$。而模仿学习的目标是训练一个策略 $\pi_{\theta}$，尽可能接近于人类专家的策略，从而更好地完成任务。

1. 行为克隆 (behavior cloning, BC)
2. 逆强化学习 (inverse RL)
3. 生成对抗模仿学习 (generative adversarial imitation learning, GAIL)

## 行为克隆

行为克隆 (BC) 就是直接使用监督学习方法，将专家数据中 $(s_t, a_t)$ 的动作样本输入，a 视为标签，学习的目标为：

$$
\theta^* = \arg \min_{\theta} \mathbb{E}_{(s,a) \sim B} [\mathcal{L}(\pi_{\theta}(s), a)]
$$

其中，B 是专家的数据集，C 是对应监督学习框架下的损失函数。若动作是离散的，该损失函数可以是最大似然估计得到的。若动作是连续的，该损失函数可以是均方误差。

在训练过程中，BC 能够很快地学习出一个不错的策略。例如，AlphaGo 就是首先在 16 万盘棋局的 3000 万个步骤子数据中学习人类选手是如何下棋的，仅仅在这个行为克隆方法，AlphaGo 的棋力就已经超过了很多业余围棋爱好者。由于 BC 的实现十分简单，因此在很多实际场景下它都可以作为策略预训练的方法。BC 能使得策略在较短的时间内获得不错的初始性能，而通过模仿专家数据可以快速收敛到较好的策略，而无需过多的探索。然而行为克隆却存在一些问题，其中一个是积累误差 (compounding error) 问题，不在此展开。

## 生成式对抗模仿学习

生成式对抗模仿学习继承了生成式对抗网络的本质逻辑和思想。GAIL 实际上是模仿学习的一种形式，与基于行为克隆的模仿学习不同，GAIL 算法中有一个判别器和一个策略，策略需要知道环境动作对 $(s, a)$ 的占用度量 $\rho_{\pi}(s, a)$ 和专家策略的占用度量一致。为了达成这一目标，策略需要和环境进行交互，以收集下一个状态的信息并进一步做出动作。这一点和 BC 不同，BC 完全不需要和环境交互。GAIL 算法中有两个判别器和一个策略，策略被当于是生成式对抗网络中的生成器 (generator)，给定一个状态，策略会输出一个状态下应该采取的动作，而判别器 (discriminator) $D$ 将状态动作对 $(s, a)$ 作为输入，输出一个 0 到 1 之间的数值，表示判别器认为动作状态对 $(s, a)$ 是来自智能体还是来自专家的数据，判别器的目的是训练出一个最优的判别器，训练目标为这会对智能体内部策略进行梯度下降。于是，利用判别器 $D$ 的损失函数为：

$$
\mathcal{L}(\phi) = -\mathbb{E}_{\rho_{\pi}} [\log D_{\phi}(s, a)] - \mathbb{E}_{\rho_E} [\log(1 - D_{\phi}(s, a))]
$$

其中 $\phi$ 是判别器的参数。对于判别器 $D$ 之后，模仿者若策略的占用频率是其交互产生的轨迹能够被判别器误认为是专家轨迹产生的轨迹。于是不用担心占用频率的偏差，我们可以用判别器 $D$ 的输出来作为奖励函数来训练策略，具体来说，若环境的状态动作对 $(s, a)$ 会输入到判别器 $D$ 中，输出 $D(s, a)$ 的值，然后将奖励设置为：

$$
r(s, a) = -\log D(s, a)
$$

于是，我们可以使用任意强化学习算法，使用这些数据训练策略，模仿者策略在成的的数据分布将会接近真实的专家分布，最终，生成器在真实环境下不能被判别器比较好的区分开来，达到模仿学习的自标。