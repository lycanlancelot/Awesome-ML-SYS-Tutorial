# 强化学习——多臂老虎机

这是 UCLA 2025 年初春的 RL 课程笔记，感谢周博磊老师的课件。我把这系列的名字其名为 Demystifying Reinforcement Learning，因为自己一直想要做一些 RLHF framework 的工作，所以对 RL 算法的直观非常重要。出于我个人的学习习惯，我对理论不感兴趣，好在对于 ML SYS researcher 而言，这样的直观已然可以带来巨大的帮助。

为了在直观和扎实的理论之间做权衡，一部分内容读上去似乎会显然到觉得愚蠢。**然而曾有一位我的 co-author 告诉我，许多平凡到看似愚蠢的定义反而是 ML Theory 的核心**。我虽然不会 ML Theory，但是心中只有谦卑，故而不会略去这些平凡的定义。

最后，这系列笔记在我的 Awesome-ML-Sys 仓库中，自己也会从 SYS 角度思考一些问题，督促自己学习。

## Introduction

- **RL 和 supervised learning 的区别**

老实说这是我本科就没有想明白的内容，老师的课件此处写的极好。

1. 输入的数据是序列的，先后顺序有重大影响
2. learner 并不会被告知做出如何的 action，需要自己去发现最终收益最大的 action
3. trial and error exploration，需要在 exploration（探索新策略）和 exploitation（利用当前策略）之间权衡
4. 不存在监督者，只有 reward 信号，而且 reward 信号是延后的

> 与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。

- **强化学习的性质**

1. trial and error exploration
2. delayed reward
3. time matters, sequential decision making, not i.i.d.
4. Actions affect the evironment

- **RL 的可能问题**

1. interpretability
2. diversity of the environment
3. overfitting on training environment
4. reward engineering
5. no safty guarantee
6. low sample efficiency

## RL Basic

### RL 的常见要素

1. Agent 和 Environment
2. State
3. Observation：注意 state 和 observation 会在[下一个段落](#Seqential-Decision-Making)做出区分
4. Reward：reward 是环境给予的标量反馈，表征当前时间步 t 下，agent 的表现如何；所有强化学习的目标都可以被概述为最大化累计期望 reward

### Seqential Decision Making

Agent 的目标是选择一系列的 action 以最大化累计期望 reward，其所选择的 Actions 需要具有长时间的影响。Reward 往往是延迟的，而 Agent 需要在即刻 reward 和长期 reward 之间做出权衡。

- History（历史，$H_t$）：历史是一组由观察（Observations）、行动（Actions）和奖励（Rewards）组成的序列，记作 
$H_t = O_1, A_1, R_1, O_2, A_2, R_2, \ldots, O_t, A_t, R_t$。它记录了智能体与环境从时间 1 到 t 的全部交互过程。虽然在完全可观察的 MDP 中，未来的状态和奖励只依赖当前状态和行动，不需要完整的 $H_t$，但历史可以作为智能体的学习记录或调试工具。

- Environment State（环境状态，$S_{t}^{e}$）：环境中某个时刻的真实、完整状态，描述了环境的所有相关信息（比如迷宫中的所有位置、墙壁、出口等）。在 MDP 中，智能体可以直接观察到 $S_{t}^{e}$，且状态转移只依赖当前 $S_{t}^{e}$ 和行动 $A_t$，不依赖历史。

- Observation（观察，$O_t$）：智能体从环境中接收到的信息。在完全可观察的 MDP 中，观察等于环境状态，即 $O_t = S_{t}^{e}$，因为环境是完全可观察的，智能体可以看到环境的全部内容。

- Agent State（智能体状态，$S_{t}^{a}$）：智能体对环境的表示。在完全可观察的 MDP 中，智能体状态直接等于环境状态，即 $S_{t}^{a} = O_t = S_{t}^{e}$，因为智能体可以直接看到环境的真实状态。

### RL Agent 的主要组成

1. Model（模型）

模型预测环境下一步将会发生什么，可以预测下一步的环境的 state（状态），或者环境的 reward（奖励）。

2. Policy（策略）

模型预测好下一步将会发生什么后，策略负责采取具体的行动，分为随机策略（Stochastic Policy）和确定性策略（Deterministic Policy）。

3. Value Function（价值函数）

价值函数用于评估 state（状态）或 state-action pair（状态-行动对）在特定策略下的长期回报，间接判断怎样的策略更优。具体有两种形式：

- State Value Function（状态价值函数）用于评估某个状态 $s$ 有多好，表示在状态 $s$ 下遵循策略 $\pi$ 的期望折现奖励。公式为：

$$
v_{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]
$$

其中，$G_t$ 是从时间 $t$ 开始的未来奖励总和，$R_{t+k+1}$ 是每个时间步的奖励，$\gamma$ 是折扣因子，用于衡量长期奖励的重要性。

- Q-function（Q 函数，状态-行动价值函数）用于评估在状态 $s$ 下采取行动 $a$ 有多好，表示在状态 $s$ 下采取行动 $a$ 遵循策略 $\pi$ 的期望折现奖励，用于在多个行动之间做出选择。公式为：

$$
q_{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a] = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]
$$

价值函数（无论是 $V(s)$ 还是 $Q(s,a)$）的核心作用是评估状态或状态-行动对在特定策略下的长期回报，而不是直接评估策略本身。它为优化策略提供了基础，通过比较不同策略下的价值函数，可以选择表现更好的策略。

## RL 的分类

### Value-Based（基于价值的）

- **定义**：Value-Based Agent 主要通过学习“价值函数”来做出决策。价值函数衡量某个状态（或状态-行动对）的好坏，表示如果从那个状态开始，按照某种策略行动，可以获得的预期累积奖励。例如，$V(s)$ 是状态 $s$ 的价值，$Q(s, a)$ 是执行动作 $a$ 在状态 $s$ 下的价值。
- **特点**：代理不直接学习“怎么做”（策略），而是学习“做什么最好”（价值的评估）。然后根据价值函数推导出隐式的策略（比如选择价值最高的动作）。
- **学习目标**：学习准确的价值函数（比如 $Q(s, a)$），以便知道每个状态或动作对的长期回报。最终通过价值函数间接生成隐式策略。
- **例子**：Q-Learning 和 Deep Q-Network（DQN）都是典型的 Value-Based 方法。它们学习 $Q$ 值表或函数，然后根据 $Q$ 值选择动作。

### Policy-Based（基于策略的）

- **定义**：Policy-Based Agent 直接学习一个策略（policy），即从状态到动作的映射（可以是概率分布）。策略可以是显式的，比如“在状态 $s$ 下，80% 概率选择动作 $a_1$，20% 概率选择 $a_2$”。
- **特点**：Agent 不依赖价值函数，而是直接优化策略，使其最大化累积奖励。策略可以是参数化的（如神经网络），通过梯度下降等方式调整。
- **学习目标**：直接学习最优策略（比如 $\pi(a|s)$，表示在状态 $s$ 下选择动作 $a$ 的概率），使得长期奖励最大化。不需要价值函数（虽然有时候会结合使用）。
- **例子**：REINFORCE 算法是典型的 Policy-Based 方法，直接优化策略参数。

### Actor-Critic（演员-评论家）
- **特点**：Actor-Critic 结合了 Value-Based 和 Policy-Based 的优点。它有两个部分：
  - **Actor**：负责学习策略（类似 Policy-Based），直接输出动作或动作概率。
  - **Critic**：负责学习价值函数（类似 Value-Based），评估当前策略的好坏（比如 $Q(s, a)$ 或 $V(s)$），指导 Actor 改进。
- **学习目标**：同时学习策略（Actor）和价值函数（Critic），目标是优化策略以最大化长期奖励，同时用价值函数提供更准确的反馈。
- **例子**：A3C（Asynchronous Advantage Actor-Critic）或 PPO（Proximal Policy Optimization）都是常见的 Actor-Critic 方法。


### Model-Based（基于模型的）

- **定义**：Model-Based Agent 通过学习环境的模型来做出决策。模型包括状态转移规则（比如“如果执行动作 $a$ 在状态 $s$，会转移到状态 $s'$”）和奖励函数（“执行这个动作能得到多少奖励”）。通俗来说，这就像是旅行之前先做了旅行攻略，把游玩路线基本都决定好了。
- **特点**：代理不仅学习策略或价值，还学习环境的内部结构（模型）。有了模型后，可以通过规划（planning）或模拟来选择最佳动作，而不完全依赖试错。
- **学习目标**：学习准确的环境模型（状态转移和奖励），然后利用模型优化策略或价值函数，以最大化长期奖励。
- **例子**：使用动态规划（Dynamic Programming）或 AlphaGo 的树搜索（Monte Carlo Tree Search, MCTS）时，模型是关键，用来预测未来的状态并指导行动选择。

### Model-Free（无模型的）

- **定义**：Model-Free Agent 不学习环境模型，而是直接从经验（试错）中学习策略或价值函数。它们只关心当前观察到的状态、动作和奖励，不需要预测未来的状态转移或奖励。类似于在一个城市随机旅行，没有提前的攻略。
- **特点**：简单直接，计算成本低，但可能需要更多的样本（经验）来学习，因为缺乏环境结构的指导。
- **学习目标**：直接学习策略（Policy-Based）或价值函数（Value-Based），以最大化累积奖励，而不依赖环境模型。
- **例子**：Q-Learning（Value-Based）、REINFORCE（Policy-Based）、PPO 或 DQN 都是 Model-Free 方法。

| 类型           | 学什么                     | 模型       | 价值函数       | 策略       | 优点                          | 缺点                          |
|----------------|-----------------------------|-------------------|-----------------------|-------------------|-----------------------------|-----------------------------|
| Value-Based    | 价值函数（间接推导策略）     | 不需要             | 是                   |隐式         | 简单，样本效率高             | 策略可能不灵活               |
| Policy-Based   | 策略                         | 不需要             | 否                   | 是                 | 策略灵活，可处理连续动作     | 样本效率低，训练不稳定       |
| Actor-Critic   | 策略 + 价值函数             | 不需要             | 是                   | 是                 | 结合两者的优点，稳定高效     | 复杂，调参难度高             |
| Model-Based    | 环境模型 + 策略/价值         | 需要               | 可选                 | 可选               | 样本效率高，能规划           | 模型学习困难，计算成本高     |
| Model-Free     | 策略或价值函数               | 不需要             | 可选                 | 可选               | 简单，易实现                 | 样本效率低，依赖大量试错     |

## 多臂老虎机问题

强化学习关注智能体和环境交互过程中的学习，这是一种试错型学习（trial-and-error learning）范式。而多臂老虎机问题是简化的强化学习。多臂老虎机不存在状态信息，只有动作和奖励，算是最简单的“和环境交互中的学习”的一种形式。

对多臂老虎机问题而言，每个杆的获奖概率是一定的，但是对 agent 未知。agent 需要做的是在每个时间步选择一个杆，然后根据杆的获奖概率获得奖励，最终使得累积奖励最大化。一个最简单的策略就是一直采取第一个动作，但这就非常依赖运气的好坏。如果运气绝佳，可能拉动的刚好是能获得最大期望奖励的拉杆，即最优拉杆；但如果运气很糟糕，获得的就有可能是最小的期望奖励。可见，探索与利用的平衡存在严重冲突。探索（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，我们要把所有的拉杆都拉动一下（甚至是很多下）才知道哪根拉杆可能获得最大的奖励。利用（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。例如，对于一个 10 臂老虎机，我们只拉动过其中 3 根拉杆，接下来就一直拉动这 3 根拉杆中期望奖励最大的那根拉杆，但很有可能期望奖励最大的拉杆在剩下的 7 根当中，即使我们对 10 根拉杆各自都尝试了 20 次，发现 5 号拉杆的经验期望奖励是最高的，但仍然存在着微小的概率—另一根 6 号拉杆的真实期望奖励是比 5 号拉杆更高的。

为了对 bandit 问题有更深入的理解，这里引入懊悔（regret）的概念：拉动当前拉杆 $\alpha$ 获得的期望奖励与最优拉杆的期望奖励的差，即 $R(\alpha) = Q^* - Q{(\alpha)}$。累积懊悔（cumulative regret）即操作 $t$ 次拉杆后累积的懊悔总量，对于一次完整的步决策，累积懊悔为 $\sigma_R = \sum_{i=1}^{t} (Q^* - Q{(\alpha_t)})$。最大化累积奖励等价于最小化累积懊悔。

### $\epsilon$-贪心算法

Q(a) 是行动（action a）的值函数，表示如果选择某个行动 a，智能体期望获得的平均奖励。具体来说，$Q_t(a)$ 是到第 $t$ 步为止，每次选择行动 $a$ 获得的奖励总和，除以选择 $a$ 的次数。这是一个基于经验的平均值，用来估计行动 $a$ 的“好坏”。

$$
  Q_t(a) = \frac{\text{sum of rewards when action a was taken prior to t}}{\text{number of times a was taken prior to t}}
$$

举例来说，行动 A 有时给你 10 分，有时给你 2 分。你尝试了行动 A 5 次，总共得到 40 分。那么 \( Q(A) = 40 / 5 = 8 \) 分。这是行动 A 的当前估计价值。

有了 $Q_t(a)$ 的定义，我们可以定义贪婪策略（Greedy Strategy），也即每次选择 $Q_t(a)$ 最大的行动 $a$。这种策略其实就是 local minima，掉入了 Exploration-Exploitation Dilemma。为此，$\epsilon$-贪婪策略引入了探索。它在大多数时候使用贪婪策略选择最佳行动，但以很小的概率 $\epsilon$ 随机选择一个行动（从所有可能的行动中均匀选择）。

这里可以给出一段经典的伪代码。考虑一个有 k 个拉杆（arms 或 actions）的老虎机，智能体需要决定每次拉哪个拉杆以最大化奖励。ε-贪婪策略在这里用来平衡探索和利用。

```plaintext
1: for a = 1 to k do
2:    Q(a) = 0, N(a) = 0
3: end for
4: loop
5:    A = {
         arg max_a Q(a)    with probability 1 - ε
         uniform(A)        with probability ε
       }
6:    r = bandit(A)
7:    N(A) = N(A) + 1
8:    Q(A) = Q(A) + 1/N(A) [r - Q(A)]
9: end loop
```

1 到 3 进行初始化，从零开始学习每个行动的价值。4 到 9 是主循环：

- 行动选择：按照前文所述的方式，进行 $\epsilon$-greedy 选择。
- 获取奖励：选择了行动 A 后，拉下拉杆，获得奖励 r。
- 更新选择次数：每次选择行动 A，将其被选择次数 $N(A)$ 增加 1。来跟踪每个行动被尝试的频率。
- 更新行动价值：使用增量式更新公式来调整 $Q(A)$，基于新获得的奖励 $r$ 和当前估计 $Q(A)$。式中，$1/N(A)$ 是“学习率”或“步长”（step size），随着 $N(A)$ 增加（即行动被选择更多次），步长变小，更新变得更平滑。$r - Q(A)$ 是“误差”或“偏差”：新奖励 $r$ 减去当前估计 $Q(A)$。如果 $r > Q(A)$，说明行动比我们估计的更好；如果 $r < Q(A)$，说明估计过高。
- 循环继续，重复选择行动、获取奖励、更新估计。

更具体的来说：

$$
  Q_t(a_t) = Q_{t-1} + \frac{1}{N_t(a_t)} (r_t - Q_{t-1}(a_t))\\
  \text{NewEstimate} = \text{OldEstimate} + \text{StepSize} \times (\text{Target} - \text{OldEstimate})
$$

这是第 8 行公式的更形式化的表达，明确了时间步（time step）的依赖。$Q_t(a_t)$ 是在时间步 $t$ 对行动 $a_t$ 的估计值。$Q_{t-1}(a_t)$ 是时间步 $t-1$ 对同一行动的估计值（旧估计）。$r_t$ 是时间步 $t$ 选择 $a_t$ 后获得的奖励。$N_t(a_t)$ 是到时间步 $t$ 为止，行动 $a_t$ 被选择的总次数。公式和第 8 行一样，表明新估计是通过旧估计加上一个基于误差 $ [r_t - Q_{t-1}(a_t)] $ 和学习率 $ \frac{1}{N_t(a_t)} $ 的调整得到的。

当然，这个多臂老虎机模型是最为简化的 RL 任务，因为奖励是不延迟的，state 不会变化，而且 action 之间不会有后续影响。

简单考虑下多臂老虎机中 $\epsilon$-greedy 策略的累积懊悔。在足够长的时间后，$\epsilon$-greedy 策略能够完整地探索到所有拉杆的期望，只要不落入 $\epsilon$ 的概率内，则一定会选择没有懊悔的全局最优拉杆。而落入 $\epsilon$ 的概率内，则会随机拉动拉杆，累积懊悔会近似为一条斜率和 $\epsilon$ 成正比的直线。

所以，我们能够料想到这样的图景：

![](./pics/epsilon-regrets.png)

倘若我们将 $\epsilon$ 设置为 $\frac{1}{t}$，则累积懊悔的表现若你所想，自然是优于固定值的 $\epsilon$-greedy 策略。

