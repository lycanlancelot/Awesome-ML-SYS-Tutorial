# 策略梯度算法

Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。策略梯度是基于策略的方法的基础，本章从策略梯度算法说起。

## 策略梯度

> 梯度策略法是 on policy 算法的。

基于策略的方法首先需要将策略参数化。假设目标策略 $\pi_{\theta}$ 是一个随机化策略，我们可以采用一个线性模型或者神经网络来表示这样一个策略模型，输入某个状态，然后输出一个动作的概率分布。我们的目标是更新一个策略在环境中的期望回报。我们将梯度算法的目标函数定义为：

$$ J(\theta) = \mathbb{E}_{s_0}\left[V^{\pi_{\theta}}(s_0)\right] $$

其中，$s_0$ 表示初始状态。现在有了目标函数，我们将目标函数对策略参数 $\theta$ 得到梯度后，就可以用梯度上升方法来最大化这个目标函数，从而得到更优策略。

$$
\begin{aligned}
\nabla_{\theta}J(\theta) &\propto \sum_{s \in S} \nu^{\pi_{\theta}}(s) \sum_{a \in A} Q^{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a|s) \\
&= \sum_{s \in S} \nu^{\pi_{\theta}}(s) \sum_{a \in A} \pi_{\theta}(a|s) Q^{\pi_{\theta}}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \\
&= \mathbb{E}_{\pi_{\theta}}\left[Q^{\pi_{\theta}}(s, a) \nabla_{\theta} \log \pi_{\theta}(a|s)\right]
\end{aligned}
$$

需要注意的是，因为上式中间期望里的下标是 $\pi_{\theta}$，所以以策略梯度算法在线策略（on-policy）算法，即必须使用当前策略 $\pi_{\theta}$ 采样的数据来计算梯度。直观理解一下策略梯度这个公式，可以发现在每一个状态下，梯度的修改是让策略更多地去采样带来高 $Q$ 值的动作，更少地去采样带来较低 $Q$ 值的动作。

## REINFORCE 算法

- 初始化学模型参数 $\theta$
- for $e = 1 \to E$：
  1. 用当前策略 $\pi_{\theta}$ 采样轨迹 $\{s_1, a_1, r_1, s_2, a_2, r_2, \ldots, s_T, a_T, r_T\}$
  2. 计算当前轨迹每个时间点往后的回报 $\sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$ 记为 $\psi_t$
  3. 对 $\theta$ 进行更新，$\theta = \theta + \alpha \sum_{t} \psi_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$
- end for

REINFORCE 算法是策略梯度乃至强化学习的典型代表，智能体根据当前策略直接和环境交互，通过采样得到的轨迹数据直接计算出策略参数的梯度，进而更新当前策略，使其向最大化策略期望回报的目标靠近。这种学习方式是典型的从交互中学习，并且其优化的目标（即策略期望回报）正是最终所使用策略的性能，这比基于价值的强化学习算法的优化目标（一般是时序差分误差的最小化）要更加直接。 REINFORCE 算法理论上是能保证局部最优的，它实际上是借助蒙特卡洛方法采样轨迹来估计动作价值，这种做法的一大优点是可以得到无偏的梯度。但是，正是因为使用了蒙特卡洛方法，REINFORCE 算法的梯度估计的方差很大，可能会造成一定程度上的不稳定，这也是将介绍的 Actor-Critic 算法要解决的问题。