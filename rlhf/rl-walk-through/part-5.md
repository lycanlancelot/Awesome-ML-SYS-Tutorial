# Dyna-Q 与 DQN 算法

在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）和无模型的强化学习（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，时序差分算法（Sarsa 和 Q-learning 算法）便是两种无模型的强化学习方法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。两种代表性的动态规划算法，即策略迭代和价值迭代，是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。这一小结介绍 Dyna-Q 算法，这也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。

强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。

## Dyna-Q

Dyna-Q 算法是非常经典的基于模型的算法，其采用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态，采取一个曾经在该状态下执行过的动作，通过模型得到转移后的状态以及奖励，并根据这个模拟数据，用 Q-learning 的更新方式来更新动作价值函数。简单来说，Dyna-Q 算法在每次迭代中，会进行一步真实采样和多步和模型进行的模拟采样，然后利用模拟采样数据和真实采样数据一起改进策略。



- 初始化 $Q(s, a)$，初始化模型 $M(s, a)$
- for 序列e = 1 -> $E$ do:
  - 得到初始状态s
  - for t = 1 -> $T$ do:
    - 用$\epsilon$-贪婪策略根据Q选择当前状态s下的动作a
    - 得到环境反馈的r，s'
    - $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$
    - $M(s, a) \leftarrow r, s'$
    - for 次数n = 1 -> $N$ do:
      1. 随机选择一个曾经访问过的状态$s_m$
      2. 获取一个曾经在状态$s_m$下执行过的动作$a_m$
      3. $r_m, s'_m \leftarrow M(s_m, a_m)$
      4. $Q(s_m, a_m) \leftarrow Q(s_m, a_m) + \alpha [r_m + \gamma \max_{a'} Q(s'_m, a') - Q(s_m, a_m)]$
    - end for
    - $s \leftarrow s'$
  - end for
- end for

可以看出，在每次环境反馈执行交互执行一次 Q-learning 之后，Dyna-Q 会进行 n 次 Q-planning，其中 Q-planning 的次数 N 是一个可调节的参数，当前为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个简单并且确定的环境中，所以当看到一条经验数据$(s, a, r, s')$时，可以直接投射模型做出更新，即$M(s, a) \leftarrow r, s'$。

## DQN

Q learning 算法需要将每个状态下所有动作 $Q$ 值的表格存储下来，当状态空间过大时，这个表格会非常大，而实际上，很多情况下，动作和状态都不是离散的，根本没法穷举。所以，我们只能通过函数来拟合估计 $Q$ 值。这里我们介绍 DQN，用于解决**连续**状态下**离散**动作的问题。

CartPole 是个非常经典的连续状态和离散动作问题，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束。智能体的状态是一个维数为 4 的向量，每一维都是连续的，其动作是离散的，只能左移或者右移，动作空间大小为 2。

假设小车的动作价值函数为 $Q(s, a)$，由于状态是连续的，无法使用表格记录，因此一个常见的解决方法是使用函数拟合（function approximation）的思想，因此我们可以用来一个神经网络来表示 $Q$。若动作是连续（无限）的，神经网络的输入是状态 $s$ 和动作 $a$，然后输出一个标量，表示在状态 $s$ 下采取动作 $a$ 能获得的价值。若动作是离散（有限）的，除了可以采取动作连接情况下的做法，我们还可以在状态 $s$ 输入到神经网络后，使其同时输出每一个动作的 $Q$ 值。通俗 $DQN$（以及 $Q$-learning）只能处理动作离散的情况，因为在图数 $Q$ 的更新过程中有 $\max_a$ 这一操作。假设神经网络用来拟合图数 $w$ 的参考是，即单个状态 $s$ 下所有可能动作 $a$ 的 $Q$ 值我们都能表示为 $Q_w(s, a)$。我们将用于拟合图数的神经网络称为 $Q$ 网络。现在来回顾 Q-learning 的更新规则以构造 Q 网络的损失函数：

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a' \in A} Q(s', a') - Q(s, a) \right]$$

上述公式用时序差分 (temporal difference, TD) 学习目标 $r + \gamma \max_{a' \in A} Q(s', a')$ 来增量式更新 $Q(s, a)$，也就是说要使 $Q(s, a)$ 和 TD 目标 $r + \gamma \max_{a' \in A} Q(s', a')$ 靠近。于是，对于一组数据[ $(s_i, a_i, r_i, s'_i)$ ]，我们可以很自然地将 Q 网络的损失函数构造为均方误差的形式：

$$\omega^* = \arg \min_{\omega} \frac{1}{2N} \sum_{i=1}^N \left[ Q_{\omega}(s_i, a_i) - \left( r_i + \gamma \max_{a'} Q_{\omega}(s'_i, a') \right) \right]^2$$

至此，我们就可以将 Q-learning 扩展到神经网络形式——深度 Q 网络 (deep Q network, DQN) 算法。由于 DQN 是 off-policy 算法，因此我们在收集数据的时候可以使用一个 $\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——经验回放和目标网络，它们能够帮助 DQN 取得稳定、出色的性能。

### 经验回放

在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了经验回放（experience replay）方法，具体做法为维护一个回放缓冲区，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用。

（1）使样本满足独立假设。在 MDP 中，交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。

（2）提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

> 注意到，Q-learning 是 off-policy 算法，但是 off-policy 算法并不意味着经验会被反复利用。

### 目标网络

DQN 算法最终更新的目标是让 $Q_w(s, a)$ 逼近 $r + \gamma \max_{a'} Q_w(s', a')$，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断被改变，这非常容易造成神经网络训练的不稳定性和震荡性。为了解决这一问题，DQN 使用了目标网络（target network）的思想：既然训练过程中 $Q$ 网络的不断更新会导致目标不断变化，不如暂时先将 TD 目标中的 $Q$ 网络固定住。为了实现这一思想，我们需要利用两套 $Q$ 网络。

1. 训练网络 $Q_w(s, a)$，用于计算原来的损失函数 $\frac{1}{2} [Q_w(s, a) - (r + \gamma \max_{a'} Q_w(s', a'))]^2$ 中的 $Q_w(s, a)$。

2. 目标网络 $Q_{\hat{w}}(s, a)$，用于计算 TD 误差目标 $r + \gamma \max_{a'} Q_{\hat{w}}(s', a')$ 的值，其中 $\hat{w}$ 表示目标网络的参数。

如果两套网络的参数都保持一致，则仍然会出现震荡的情况，因此可以定期更新目标网络的参数 $\hat{w}$，比如每隔若干步更新一次，即 $\hat{w} \leftarrow w$。这样做的目的是让目标网络的参数相对稳定，从而使得 TD 误差目标的计算稳定下来。


- 用随机初始参数 $w$ 初始化网络 $Q_w(s, a)$
- 复制相同的参数 $w^- \leftarrow w$ 来初始化目标网络 $Q_{w^-}$
- 初始化经验回放池 $R$
- for 序列 $e = 1 \to E$ do:
  - 获取环境初始状态 $s_1$
  - for 时间步 $t = 1 \to T$ do:
    1. 根据当前网络 $Q_w(s, a)$ 以 $\epsilon$-贪婪策略选择动作 $a_t$
    2. 执行动作 $a_t$，获得环境反馈 $r_t$，环境状态变为 $s_{t+1}$
    3. 将 $(s_t, a_t, r_t, s_{t+1})$ 存入经验回放池 $R$ 中
    4. 若 $R$ 中数据足够，从 $R$ 中采样 $N$ 个数据 $\{(s_i, a_i, r_i, s_{i+1})\}_{i=1,\dots,N}$
    5. for 每个数据 $i$ do:
      - 计算目标值 $y_i = r_i + \gamma \max_{a'} Q_{w^-}(s_{i+1}, a')$
      - 最小化目标损失 $L = \frac{1}{N} \sum_i (y_i - Q_w(s_i, a_i))^2$，以此更新当前网络 $Q_w$
    6. 更新目标网络
  - end for
- end for