# 动态规划算法

动态规划算法从大一入学就在学，但是我那时候一直不理解为什么会有个专有名词“价值函数”在里面，现在才发现，居然是从 RL 里面继承的名词。世界线突然收束了。

这一部分讲述用动态规划法来求解 MDP 决策过程的最优策略。基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，使其稳步上升，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。

## 策略迭代算法

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 策略评估

如同一节所讲，策略的状态价值函数的精确解是一个 $n^3$ 的暴力过程，因此我们考虑用贝尔曼期望方程来计算数值解：

$$ V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^{\pi}(s') \right) $$

其中，$\pi(a|s)$ 是策略在状态 $s$ 下采取动作 $a$ 的概率。可以看得出，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值函数计算当前状态的价值。由动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。更一般的，在第 0 轮，我们对所有状态的价值函数直接指定数值，接着，我们可以在下一轮更新所有状态的价值函数。如此以来，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即：

$$ V^{k+1}(s) = \sum_{a \in A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^{k}(s') \right) $$

根据贝尔曼期望方程，可以得知 $V^k = V^{\pi}$ 是以上更新公式的一个不动点（fixed point）。事实上，可以证明当 $k \to \infty$ 时，序列 $\{V^k\}$ 会收敛到 $V^{\pi}$。所以可以通过迭代来计算得到一个策略的状态价值图函数。可以看出，由于需要不断做贝尔曼期望更新，状态评估其实会耗费很大的计算价值。在实际的实践中，如果是一轮 $\max_{s \in S} |V^{k+1}(s) - V^k(s)|$ 的值非常小，可以提前结束评估。这种做法可以加快收敛速度，并且得到的值也非常接近真实的价值。

### 策略提升

我们可以直接贪心地在每一个状态选择状态价值函数最大的动作，也就是：

$$
\pi'(s) = \arg \max_{a} Q^{\pi}(s, a) = \arg \max_{a} \{r(s, a) + \gamma \sum_{s'} P(s'|s, a) V^{\pi}(s')\}
$$

这一定是基于当前策略评估得到的最佳策略。当然，我们的策略评估可能是不准的，所以迭代下一轮，会有新的最佳策略。二者在一起，就是策略迭代算法，不断循环去做策略评估和策略提升。

## 价值迭代算法

有别于策略迭代算法，基于贝尔曼最优方程，我们可以直接迭代得到最终价值，从而一步获得最优策略。

$$ V^*(s) = \max_{a \in A} \left\{ r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^*(s') \right\} $$

将其写成迭代更新的方式为：

$$ V^{k+1}(s) = \max_{a \in A} \left\{ r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^k(s') \right\} $$

价值迭代便是逐步按照以上更新方式进行的。等到 $V^{k+1}$ 和 $V^k$ 相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数 $V^*$。然后我们利用：

$$ \pi(s) = \arg \max_{a} \left\{ r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^{k+1}(s') \right\} $$

从中挑选出最优策略即可。其计算自然要清爽很多，迭代多轮价值，最后一轮即可得到最佳策略。
